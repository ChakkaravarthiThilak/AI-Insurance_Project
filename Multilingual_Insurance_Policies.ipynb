{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a848bd81-c574-4749-8674-30c2b6193ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Policy_Text_EN_Normalized  \\\n",
      "0  this policy covers damages to your vehicle due...   \n",
      "1  comprehensive health insurance policy covering...   \n",
      "2  home insurance protecting against fire theft a...   \n",
      "3  travel insurance providing medical and trip ca...   \n",
      "4  life insurance ensuring financial security for...   \n",
      "\n",
      "                           Policy_Text_FR_Normalized  \\\n",
      "0  cette police couvre les dommages √† votre v√©hic...   \n",
      "1  police dassurance maladie compl√®te couvrant lh...   \n",
      "2  assurance habitation prot√©geant contre les inc...   \n",
      "3  assurance voyage offrant une couverture m√©dica...   \n",
      "4  assurance vie garantissant la s√©curit√© financi...   \n",
      "\n",
      "                           Policy_Text_ES_Normalized  \\\n",
      "0  esta p√≥liza cubre da√±os a su veh√≠culo por acci...   \n",
      "1  p√≥liza de seguro de salud integral que cubre h...   \n",
      "2  seguro de hogar que protege contra incendios r...   \n",
      "3  seguro de viaje que proporciona cobertura m√©di...   \n",
      "4  seguro de vida que garantiza la seguridad fina...   \n",
      "\n",
      "                          Summarized_Text_Normalized  \n",
      "0  covers vehicle damages from accidents theft an...  \n",
      "1  health insurance covering hospitalization and ...  \n",
      "2  home insurance for fire theft and natural disa...  \n",
      "3  travel insurance with medical and trip cancell...  \n",
      "4  life insurance for financial security of depen...  \n"
     ]
    }
   ],
   "source": [
    "# üì¶ Required Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# üìÅ Load Data\n",
    "df = pd.read_csv(\"Multilingual_Insurance_Policies_50.csv\")\n",
    "\n",
    "# ‚úÖ Dictionary for English contractions\n",
    "contractions_dict = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"can't\": \"cannot\", \"won't\": \"will not\",\n",
    "    \"i'm\": \"i am\", \"they're\": \"they are\", \"it's\": \"it is\", \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\", \"we're\": \"we are\", \"you're\": \"you are\", \"i've\": \"i have\",\n",
    "    \"we've\": \"we have\", \"they've\": \"they have\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\",\n",
    "    \"there's\": \"there is\", \"that's\": \"that is\", \"what's\": \"what is\", \"who's\": \"who is\"\n",
    "}\n",
    "\n",
    "# üîÅ Contraction expansion function\n",
    "def expand_contractions(text):\n",
    "    for contraction, full in contractions_dict.items():\n",
    "        text = re.sub(rf\"\\b{contraction}\\b\", full, text)\n",
    "    return text\n",
    "\n",
    "# üîß English normalization\n",
    "def normalize_english_text(text):\n",
    "    text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# üîß French/Spanish normalization\n",
    "def normalize_text_generic(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z√Ä-√ø0-9\\s]\", \"\", text)  # preserve accents\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# üßπ Apply to English and Summary\n",
    "df[\"Policy_Text_EN_Normalized\"] = df[\"Policy_Text_EN\"].apply(normalize_english_text)\n",
    "df[\"Summarized_Text_Normalized\"] = df[\"Summarized_Text\"].apply(normalize_english_text)\n",
    "\n",
    "# üßπ Apply to French and Spanish\n",
    "df[\"Policy_Text_FR_Normalized\"] = df[\"Policy_Text_FR\"].apply(normalize_text_generic)\n",
    "df[\"Policy_Text_ES_Normalized\"] = df[\"Policy_Text_ES\"].apply(normalize_text_generic)\n",
    "\n",
    "# üíæ Save normalized data\n",
    "df.to_csv(\"Normalized_Multilingual_Insurance_Policies.csv\", index=False)\n",
    "\n",
    "# ‚úÖ View sample output\n",
    "print(df[[\n",
    "    \"Policy_Text_EN_Normalized\", \n",
    "    \"Policy_Text_FR_Normalized\", \n",
    "    \"Policy_Text_ES_Normalized\", \n",
    "    \"Summarized_Text_Normalized\"\n",
    "]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c376e66c-437e-4935-87c3-c96d5003c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\.conda\\New conda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\New conda\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a25995d36054b2993111eb2853c79ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\New conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd2148895aa44659dc97a63cbb9e90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\New conda\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8640\\1700180873.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  translation_trainer = Seq2SeqTrainer(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8640\\1700180873.py:93: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  summarization_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Fine-tuning translation model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 18:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\New conda\\Lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Fine-tuning summarization model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 16:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.929900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.139100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned models saved. Ready for Streamlit integration!\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Required Libraries\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    MarianMTModel, MarianTokenizer,\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# üìÅ Load Normalized Dataset\n",
    "df = pd.read_csv(\"Normalized_Multilingual_Insurance_Policies.csv\")\n",
    "\n",
    "# üîß Create fallback summary column if not available\n",
    "df[\"Summary_EN\"] = df[\"Policy_Text_EN_Normalized\"].apply(lambda x: x[:150])\n",
    "\n",
    "# ‚úÖ Prepare Dataset for Fine-tuning\n",
    "translation_dataset = pd.DataFrame({\n",
    "    \"translation_input\": df[\"Policy_Text_EN_Normalized\"],\n",
    "    \"translation_target\": df[\"Policy_Text_FR\"]\n",
    "})\n",
    "\n",
    "summarization_dataset = pd.DataFrame({\n",
    "    \"summary_input\": \"summarize: \" + df[\"Policy_Text_EN_Normalized\"],\n",
    "    \"summary_target\": df[\"Summary_EN\"]\n",
    "})\n",
    "\n",
    "# üîÑ Load Tokenizers & Models\n",
    "translation_model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translation_tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(translation_model_name)\n",
    "\n",
    "summarization_model_name = \"t5-small\"\n",
    "summarization_tokenizer = T5Tokenizer.from_pretrained(summarization_model_name)\n",
    "summarization_model = T5ForConditionalGeneration.from_pretrained(summarization_model_name)\n",
    "\n",
    "# ‚úÖ Dataset Conversion to Hugging Face Format\n",
    "def tokenize_translation(example):\n",
    "    model_inputs = translation_tokenizer(example[\"translation_input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with translation_tokenizer.as_target_tokenizer():\n",
    "        labels = translation_tokenizer(example[\"translation_target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "translation_hf = Dataset.from_pandas(translation_dataset)\n",
    "translation_hf = translation_hf.map(tokenize_translation, batched=True)\n",
    "\n",
    "\n",
    "def tokenize_summarization(example):\n",
    "    model_inputs = summarization_tokenizer(example[\"summary_input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with summarization_tokenizer.as_target_tokenizer():\n",
    "        labels = summarization_tokenizer(example[\"summary_target\"], max_length=60, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "summarization_hf = Dataset.from_pandas(summarization_dataset)\n",
    "summarization_hf = summarization_hf.map(tokenize_summarization, batched=True)\n",
    "\n",
    "# üèÅ Training Arguments\n",
    "translation_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./finetuned_translation_model\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=4,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "summarization_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./finetuned_summarization_model\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=4,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# üß† Define Trainers\n",
    "data_collator_translation = DataCollatorForSeq2Seq(tokenizer=translation_tokenizer, model=translation_model)\n",
    "translation_trainer = Seq2SeqTrainer(\n",
    "    model=translation_model,\n",
    "    args=translation_args,\n",
    "    train_dataset=translation_hf,\n",
    "    tokenizer=translation_tokenizer,\n",
    "    data_collator=data_collator_translation\n",
    ")\n",
    "\n",
    "data_collator_summarization = DataCollatorForSeq2Seq(tokenizer=summarization_tokenizer, model=summarization_model)\n",
    "summarization_trainer = Seq2SeqTrainer(\n",
    "    model=summarization_model,\n",
    "    args=summarization_args,\n",
    "    train_dataset=summarization_hf,\n",
    "    tokenizer=summarization_tokenizer,\n",
    "    data_collator=data_collator_summarization\n",
    ")\n",
    "\n",
    "# üöÄ Start Fine-tuning\n",
    "print(\"üöÄ Fine-tuning translation model...\")\n",
    "translation_trainer.train()\n",
    "translation_model.save_pretrained(\"./finetuned_translation_model\")\n",
    "translation_tokenizer.save_pretrained(\"./finetuned_translation_model\")\n",
    "\n",
    "print(\"üöÄ Fine-tuning summarization model...\")\n",
    "summarization_trainer.train()\n",
    "summarization_model.save_pretrained(\"./finetuned_summarization_model\")\n",
    "summarization_tokenizer.save_pretrained(\"./finetuned_summarization_model\")\n",
    "\n",
    "print(\"‚úÖ Fine-tuned models saved. Ready for Streamlit integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da8b4a-d245-4637-804a-9b0f0885f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# üì¶ Wrap translation model and tokenizer\n",
    "with open(\"translation_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model\": translation_model,\n",
    "        \"tokenizer\": translation_tokenizer\n",
    "    }, f)\n",
    "\n",
    "# üì¶ Wrap summarization model and tokenizer\n",
    "with open(\"summarization_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model\": summarization_model,\n",
    "        \"tokenizer\": summarization_tokenizer\n",
    "    }, f)\n",
    "\n",
    "print(\"‚úÖ Models saved as .pkl files for Streamlit app usage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a6cfb-e291-47fe-960f-1c9e2fc94e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
